{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b0add56",
   "metadata": {},
   "source": [
    "# Scrape Seals\n",
    "\n",
    "### Currently operable with zdic website\n",
    "\n",
    "Make sure to add characters to main data store with the add_new_batch script, before trying to scrape their images.\n",
    "\n",
    "If scraping on characters listed in missing_chars.csv, set retry = True. If scraping on all characters, set retry = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6cad38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import cssutils\n",
    "from PIL import Image\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import io\n",
    "from wand.api import library\n",
    "import wand.color\n",
    "import wand.image\n",
    "import time\n",
    "import sys\n",
    "\n",
    "data_dir = './source'\n",
    "# scrape_url_base = 'https://hanziyuan.net/#'\n",
    "source = {\n",
    "    1: 'https://www.zdic.net/hans/',\n",
    "    2: 'https://www.cidianwang.com/shuowenjiezi/'\n",
    "}\n",
    "scrape_url_base = 'https://www.zdic.net/hans/'\n",
    "new_image_filetype = 'png'\n",
    "log = 'log.txt'\n",
    "retry = False\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, log)):\n",
    "    log = open(os.path.join(data_dir, log), 'x', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e1763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    char_array = np.genfromtxt(os.path.join(data_dir, file), delimiter=',', encoding='utf8', dtype=None) \n",
    "    print(f\"Imported {char_array.shape[0]} characters \")\n",
    "    \n",
    "    return char_array\n",
    "\n",
    "def log_error(error):\n",
    "    with open(os.path.join(data_dir, log), 'a', encoding='utf8') as f:\n",
    "        f.write(error + '\\n')\n",
    "        \n",
    "        f.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16072d0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the image url from the site for given character\n",
    "def scrape_image(url, char, index):\n",
    "    url_list = []\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find_all(\"img\", {\"class\": \"lazy kxtimg\"})\n",
    "    for x in results:\n",
    "        attr = x.attrs['data-original']\n",
    "        \n",
    "        # Search for different variants\n",
    "        if ('swxz' in attr): # or ('swdz' in attr):\n",
    "            print(attr)\n",
    "            url_list.append(attr)\n",
    "    if len(url_list) == 0:\n",
    "        log_error(f'Char: {index}. Could not locate seal script image - maybe does not exist')\n",
    "        return None\n",
    "    \n",
    "    return url_list\n",
    "\n",
    "# Website is dynamic so path needs to be manually changed\n",
    "def recreate_img_path(path):\n",
    "    inilist = [m.start() for m in re.finditer(r\"/\", path)]\n",
    "    try:\n",
    "        new_path = 'https:' + path[0:inilist[2]+1] + 'swxz' + path[inilist[3]:]\n",
    "        \n",
    "        return new_path\n",
    "    except:\n",
    "        print('Caught error while changing paths')\n",
    "        print('HTML structure may have changed (images no longer located in swxz folder or url changed), needs diagnosis')\n",
    "        \n",
    "def save_image(url_list, files, index, curr_char_dir):\n",
    "#       seal_image_url = recreate_img_path(img_path) # Url of image\n",
    "#           print(f'URL of image: {seal_image_url}')\n",
    "    curr_image_count = str(int(files[-1][files[-1].index('_')+1:files[-1].index('.')])) if not len(files) == 0 else '1'     \n",
    "    print(url_list)\n",
    "    for url in range(len(url_list)):\n",
    "        img_path = 'https:' + url_list[url]\n",
    "        new_image_variant_number =  str(int(curr_image_count) + url)\n",
    "#         data = requests.get(img_path).content # Get svg image\n",
    "\n",
    "        # Get the filename and full path of the new image of currently scraped character\n",
    "        new_image_filename = str(index) + '_' + new_image_variant_number\n",
    "        new_image_path = f'{os.path.join(curr_char_dir, new_image_filename)}.{new_image_filetype}'\n",
    "        print(f'Filename for new image is: {new_image_filename}')\n",
    "\n",
    "\n",
    "        # Retrieve SVG file from url obtained from scraping and save SVG file\n",
    "        svg_filename, headers = urllib.request.urlretrieve(img_path, os.path.join(curr_char_dir, f\"{new_image_filename}.svg\"))#img_path[img_path.rfind('/')+1:]))\n",
    "        print(svg_filename)\n",
    "\n",
    "        with open(svg_filename, \"r\") as f:\n",
    "            svg_blob = f.read().encode('utf-8')\n",
    "            with wand.image.Image( blob=svg_blob, format=\"svg\" ) as image:\n",
    "                png_image = image.make_blob(\"png\")\n",
    "                fp = io.BytesIO(png_image)\n",
    "                with fp:\n",
    "                    img = mpimg.imread(fp, format='png')\n",
    "                    plt.imshow(img, cmap='gray')\n",
    "                    plt.axis('off')\n",
    "                    plt.savefig(new_image_path)\n",
    "\n",
    "                    fp.close()\n",
    "                f.close()\n",
    "            os.remove(svg_filename)\n",
    "\n",
    "            print(f'Successfully saved new image at {new_image_filename}')\n",
    "        time.sleep(3)\n",
    "\n",
    "def scrape_seal(index, char):\n",
    "    try:\n",
    "        print(f\"Searching for character {index}: {char}\")\n",
    "        scrape_url = scrape_url_base + char\n",
    "        print(f\"Searching on URL {scrape_url}\")\n",
    "\n",
    "        curr_char_dir = os.path.join(data_dir, str(index))\n",
    "        files = os.listdir(curr_char_dir)\n",
    "        files = [f for f in files if os.path.isfile(curr_char_dir+'/'+f)]\n",
    "\n",
    "        url_list = scrape_image(scrape_url, char, index)\n",
    "\n",
    "        # Save each image of the character\n",
    "        if url_list != None:\n",
    "            print(f'Found {len(url_list)} images for character')\n",
    "    #             print(url_list)\n",
    "            save_image(url_list, files, index, curr_char_dir)\n",
    "    \n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        msg = f'Failed to obtain char {index}, with error: {e}'\n",
    "        log_error(msg)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7bfdb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN FUNCTION FOR SCRAPING\n",
    "def main(retry):\n",
    "    if retry:\n",
    "        missed_chars = load_data('missing_chars.csv') # Import characters to scrape\n",
    "    else:\n",
    "        char_array = load_data('hsk.csv')\n",
    "    for char in range(char_array.shape[0]):\n",
    "        # Get index and character symbol from batch array\n",
    "        curr_index = char_array[char][0]\n",
    "        curr_char = char_array[char][1]\n",
    "        if retry:\n",
    "             if str(curr_index) in missed_chars:\n",
    "                scrape_seal(curr_index, curr_char)\n",
    "        else:\n",
    "            scrape_seal(curr_index, curr_char)\n",
    "    print()\n",
    "    print(\"Finished scraping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca783ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(retry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe85bbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.zdic.net/hans/é«˜'\n",
    "# page = requests.get(url)\n",
    "# soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "# results = soup.find_all(\"img\", {\"class\": \"lazy kxtimg\"})\n",
    "# for x in results:\n",
    "#     print(x)\n",
    "#     attr = x.attrs['data-original']\n",
    "#     print()\n",
    "#     if ('swxz' in attr) or ('swdz' in attr):\n",
    "#         print(\"FOUND IT\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
