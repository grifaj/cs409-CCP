{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6cad38c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 177 characters\n",
      "['起' '果' '热' '院' '四' '回' '西' '国' '高' '怎' '系' '北' '一' '七' '上' '下' '不' '东'\n",
      " '子' '医' '字' '个' '十' '中' '午' '气' '学' '么' '开' '水' '九' '习' '书' '汉' '雨' '买'\n",
      " '见' '视' '零' '觉' '了' '二' '在' '五' '些' '生' '坐' '电' '块' '影' '很' '校' '客' '样'\n",
      " '家' '桌' '去' '京' '亮' '想' '友' '人' '没' '什' '对' '今' '叫' '他' '们' '面' '候' '苹'\n",
      " '做' '小' '少' '吃' '同' '名' '后' '吗' '听' '会' '爱' '爸' '租' '呢' '住' '岁' '作' '你'\n",
      " '和' '茶' '椅' '的' '车' '站' '那' '钟' '哪' '狗' '认' '识' '话' '日' '都' '语' '时' '菜'\n",
      " '说' '请' '读' '儿' '先' '钱' '我' '八' '六' '老' '关' '兴' '这' '再' '写' '打' '商' '猫'\n",
      " '谁' '明' '星' '昨' '飞' '谢' '是' '喂' '多' '喜' '喝' '大' '天' '冷' '看' '太' '几' '出'\n",
      " '女' '她' '好' '睡' '妈' '工' '现' '衣' '能' '里' '分' '饭' '馆' '姐' '前' '欢' '师' '脑'\n",
      " '火' '漂' '月' '有' '朋' '服' '年' '期' '本' '米' '店' '机' '来' '杯' '点']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "data_dir = './source'\n",
    "scrape_url_base = 'https://hanziyuan.net/#'\n",
    "\n",
    "char_array = np.genfromtxt(os.path.join(data_dir, 'hsk_1_formatted.txt'), delimiter='', encoding='utf8', dtype=None)\n",
    "print(f\"Imported {char_array.shape[0]} characters\")\n",
    "print(char_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c112290",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m page \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(scrape_url)\n\u001b[0;32m      4\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(page\u001b[38;5;241m.\u001b[39mcontent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m result \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124metymologyS\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "for char in range(1):#char_array.shape[0]):\n",
    "    scrape_url = scrape_url_base + char_array[char]\n",
    "    page = requests.get(scrape_url)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    result = soup.find_all('div', class_=re.compile('etymologyS'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
