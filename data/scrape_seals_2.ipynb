{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d6bfeea",
   "metadata": {},
   "source": [
    "# Scrape seals\n",
    "\n",
    "### Operates on www.cidianwang.com/shouwenjiezi\n",
    "\n",
    "Finds images for characters listed in missing_chars.csv (characters without images).\n",
    "When an image for a character cannot be found on zdic.net (scrape_seals.ipynb), this script is used due to its larger repo of images, but is slower to execute due to the website structure.\n",
    "\n",
    "If scraping a manually input array, change *manual* to True in first cell, and update *manual_url* array. Structure of array is [[character (string), url (string), character index (int)], ...]\n",
    "\n",
    "Error messages (e.g. cannot find image on website) are logged to **log_2.txt**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0a87755",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import re\n",
    "import cssutils\n",
    "from PIL import Image\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import io\n",
    "from wand.api import library\n",
    "import wand.color\n",
    "import wand.image\n",
    "import time\n",
    "import sys\n",
    "import httplib2\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "data_dir = './source'\n",
    "# scrape_url_base = 'https://hanziyuan.net/#'\n",
    "\n",
    "scrape_url_base = 'https://www.cidianwang.com/shuowenjiezi/'\n",
    "new_image_filetype = 'png'\n",
    "log = 'log_2.txt'\n",
    "retry = True\n",
    "\n",
    "manual = False\n",
    "\n",
    "if not os.path.exists(os.path.join(data_dir, log)):\n",
    "    log = open(os.path.join(data_dir, log), 'x', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7d9ad075",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_url = [[\"系\", \"https://www.cidianwang.com/shuowenjiezi/xi2817.htm\", 10],\n",
    "             [\"狗\",\"https://www.cidianwang.com/shuowenjiezi/gou2295.htm\",100],\n",
    "             [\"饼\",\"https://www.cidianwang.com/shuowenjiezi/bing4344.htm\", 1013],\n",
    "             [\"识\", \"https://www.cidianwang.com/shuowenjiezi/shi3568.htm\", 102],\n",
    "             [\"歉\", \"https://www.cidianwang.com/shuowenjiezi/qian1873.htm\", 1029],\n",
    "             [\"排\", \"https://www.cidianwang.com/shuowenjiezi/pai1474.htm\", 1046],\n",
    "             [\"材\", \"https://www.cidianwang.com/shuowenjiezi/cai1677.htm\", 1065],\n",
    "             [\"骄\", \"https://www.cidianwang.com/shuowenjiezi/jiao4375.htm\", 1068],\n",
    "             [\"阅\", \"https://www.cidianwang.com/shuowenjiezi/yue4154.htm\", 1075],\n",
    "             [\"再\", \"https://www.cidianwang.com/shuowenjiezi/zai319.htm\", 122],\n",
    "             [\"点\", \"https://www.cidianwang.com/shuowenjiezi/dian2212.htm\", 177],\n",
    "             [\"丈\", \"https://www.cidianwang.com/shuowenjiezi/zhang31.htm\", 188],\n",
    "             [\"汽\", \"https://www.cidianwang.com/shuowenjiezi/qi1945.htm\", 210],\n",
    "             [\"踢\", \"\", 215],\n",
    "             [\"洗\", \"https://www.cidianwang.com/shuowenjiezi/xi2011.htm\", 244],\n",
    "             [\"唱\", \"https://www.cidianwang.com/shuowenjiezi/chang624.htm\", 293],\n",
    "             [\"报\", \"https://www.cidianwang.com/shuowenjiezi/bao1406.htm\", 310],\n",
    "             [\"睛\", \"\", 312],\n",
    "             [\"糖\", \"https://www.cidianwang.com/shuowenjiezi/tang2812.htm\", 366],\n",
    "             [\"根\", \"https://www.cidianwang.com/shuowenjiezi/gen1760.htm\", 418],\n",
    "             [\"参\", \"\", 424],\n",
    "             [\"选\", \"https://www.cidianwang.com/shuowenjiezi/xuan3876.htm\", 460],\n",
    "             [\"辆\", \"\", 494],\n",
    "             [\"邮\", \"https://www.cidianwang.com/shuowenjiezi/you3936.htm\", 495],\n",
    "             [\"调\", \"https://www.cidianwang.com/shuowenjiezi/tiao3620.htm\", 537],\n",
    "             [\"裤\", \"\", 604],\n",
    "             [\"孤\", \"https://www.cidianwang.com/shuowenjiezi/gu921.htm\", 672],\n",
    "             [\"乓\", \"\", 685],\n",
    "             [\"熟\", \"\", 715],\n",
    "             [\"忆\", \"\", 771],\n",
    "             [\"倍\", \"https://www.cidianwang.com/shuowenjiezi/bei234.htm\", 772],\n",
    "             [\"擦\", \"\", 775],\n",
    "             [\"龄\", \"https://www.cidianwang.com/shuowenjiezi/ling4540.htm\", 776],\n",
    "             [\"尊\", \"https://www.cidianwang.com/shuowenjiezi/zun988.htm\", 789],\n",
    "             [\"速\", \"https://www.cidianwang.com/shuowenjiezi/su3889.htm\", 820],\n",
    "             [\"堵\", \"https://www.cidianwang.com/shuowenjiezi/du763.htm\", 845],\n",
    "             [\"租\", \"https://www.cidianwang.com/shuowenjiezi/zu2656.htm\", 85],\n",
    "             [\"植\", \"https://www.cidianwang.com/shuowenjiezi/zhi1811.htm\", 853],\n",
    "             [\"咳\", \"https://www.cidianwang.com/shuowenjiezi/ke590.htm\", 860],\n",
    "             [\"针\", \"https://www.cidianwang.com/shuowenjiezi/nie10040.htm\", 864],\n",
    "             [\"住\", \"\", 87],\n",
    "             [\"许\", \"https://www.cidianwang.com/shuowenjiezi/xu3557.htm\", 880],\n",
    "             [\"族\", \"https://www.cidianwang.com/shuowenjiezi/zu1580.htm\", 885],\n",
    "             [\"钥\", \"\", 906],\n",
    "             [\"盐\", \"https://www.cidianwang.com/shuowenjiezi/yan2509.htm\", 914],\n",
    "             [\"聊\", \"https://www.cidianwang.com/shuowenjiezi/liao3014.htm\", 935],\n",
    "             [\"够\", \"\", 958],\n",
    "             [\"那\", \"https://www.cidianwang.com/shuowenjiezi/na3932.htm\", 97],\n",
    "             [\"释\", \"https://www.cidianwang.com/shuowenjiezi/shi4022.htm\", 999]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8d9493ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def log_error(error):\n",
    "    with open(os.path.join(data_dir, log), 'a', encoding='utf8') as f:\n",
    "        f.write(error + '\\n')\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "def load_data(csv):\n",
    "    data = np.genfromtxt(os.path.join(data_dir, csv), delimiter=',', encoding='utf8', dtype=None)\n",
    "    print(f\"Imported {data.shape[0]} characters from index {csv}\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "def scrape_image(url, char, index):\n",
    "    try:\n",
    "        headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"}\n",
    "        page = requests.get(url, headers=headers) \n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        results = soup.find(\"img\", {\"class\": \"jzimg\"})\n",
    "\n",
    "        print(results.attrs[\"src\"])\n",
    "\n",
    "        opener = urllib.request.build_opener()\n",
    "        opener.addheaders = [('User-Agent', \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\")]\n",
    "        urllib.request.install_opener(opener)\n",
    "\n",
    "        gif_filename, headers = urllib.request.urlretrieve(results.attrs[\"src\"], os.path.join(data_dir, str(index), f\"{index}_1.gif\"))#img_path[img_path.rfind('/')+1:]))\n",
    "        print(gif_filename)\n",
    "\n",
    "        format_save_image(gif_filename)\n",
    "#         plt.imshow(img)\n",
    "#         plt.show()\n",
    "    except:\n",
    "        e = sys.exc_info()[0]\n",
    "        msg = f'Char {index}. Failed to obtain with error: {e}'\n",
    "        print(msg)\n",
    "        log_error(msg)\n",
    "\n",
    "def get_page_urls(url, data):\n",
    "\n",
    "    url_char_list = []\n",
    "    \n",
    "    # ADD BROWSER HEADER TO AVOID 403 FORBIDDEN HTML ERROR\n",
    "    headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"}\n",
    "    page = requests.get(url, headers=headers) \n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    results = soup.find_all(\"a\")\n",
    "\n",
    "    for r in results:\n",
    "        if \"title\" in r.attrs:\n",
    "            char = r.attrs[\"title\"][0]\n",
    "            if char in data['f0']: # Title of first column of data np.array\n",
    "                char_link = r.attrs[\"href\"]\n",
    "                search_url = scrape_url_base + char_link[char_link.index(\"/\", 2)+1:]\n",
    "                url_char_list.append([char, search_url])\n",
    "                \n",
    "    # Add character index to [char, url]            \n",
    "    for x in range(len(data)):\n",
    "        for y in range(len(url_char_list)):\n",
    "            if data[x][0] == url_char_list[y][0]:\n",
    "                url_char_list[y].append(data[x][1])\n",
    "                break\n",
    "    \n",
    "    return url_char_list\n",
    "\n",
    "# Zooms in to centre of image\n",
    "def clipped_zoom(img, zoom_factor, **kwargs):\n",
    "\n",
    "    h, w = img.shape[:2]\n",
    "\n",
    "    # For multichannel images we don't want to apply the zoom factor to the RGB\n",
    "    # dimension, so instead we create a tuple of zoom factors, one per array\n",
    "    # dimension, with 1's for any trailing dimensions after the width and height.\n",
    "    zoom_tuple = (zoom_factor,) * 2 + (1,) * (img.ndim - 2)\n",
    "\n",
    "    # Zooming out\n",
    "    if zoom_factor < 1:\n",
    "\n",
    "        # Bounding box of the zoomed-out image within the output array\n",
    "        zh = int(np.round(h * zoom_factor))\n",
    "        zw = int(np.round(w * zoom_factor))\n",
    "        top = (h - zh) // 2\n",
    "        left = (w - zw) // 2\n",
    "\n",
    "        # Zero-padding\n",
    "        out = np.zeros_like(img)\n",
    "        out[top:top+zh, left:left+zw] = zoom(img, zoom_tuple, **kwargs)\n",
    "\n",
    "    # Zooming in\n",
    "    elif zoom_factor > 1:\n",
    "\n",
    "        # Bounding box of the zoomed-in region within the input array\n",
    "        zh = int(np.round(h / zoom_factor))\n",
    "        zw = int(np.round(w / zoom_factor))\n",
    "        top = (h - zh) // 2\n",
    "        left = (w - zw) // 2\n",
    "\n",
    "        out = zoom(img[top:top+zh, left:left+zw], zoom_tuple, **kwargs)\n",
    "\n",
    "        # `out` might still be slightly larger than `img` due to rounding, so\n",
    "        # trim off any extra pixels at the edges\n",
    "        trim_top = ((out.shape[0] - h) // 2)\n",
    "        trim_left = ((out.shape[1] - w) // 2)\n",
    "        out = out[trim_top:trim_top+h, trim_left:trim_left+w]\n",
    "\n",
    "    # If zoom_factor == 1, just return the input array\n",
    "    else:\n",
    "        out = img\n",
    "    return out\n",
    "\n",
    "# Return image im with size shape_from after padding with 255 (white) to size shape_to\n",
    "def pad_image(shape_to, shape_from, im):\n",
    "    # Amount of pixels to pad before and after rotated image in x and y directions, to return to original size\n",
    "    pad_x = int(np.ceil((shape_to[1]-shape_from[1])/2))\n",
    "    pad_y = int(np.ceil((shape_to[0]-shape_from[0])/2))\n",
    "    \n",
    "    padded_image = np.pad(im, ((500,500),(500,500)), 'constant', constant_values=(255))\n",
    "\n",
    "#     print(padded_image.shape[0]//2-shape_to[0]//2,padded_image.shape[0]//2+shape_to[0]//2)\n",
    "    cropped_image = padded_image[padded_image.shape[0]//2-shape_to[0]//2:padded_image.shape[0]//2+shape_to[0]//2, padded_image.shape[1]//2-shape_to[1]//2:padded_image.shape[1]//2+shape_to[1]//2]\n",
    "    \n",
    "    return cropped_image\n",
    "\n",
    "def format_save_image(filename):\n",
    "    img = Image.open(filename).convert('L')\n",
    "    \n",
    "    im = np.asarray(img)\n",
    "#     print(im.shape)\n",
    "#     print(im)\n",
    "\n",
    "    # im is read-only\n",
    "    im = im.copy()\n",
    "\n",
    "    # remove watermark through thresholding\n",
    "    thres = 140\n",
    "    im[im > thres] = 255\n",
    "    im[im <= thres] = 0\n",
    "\n",
    "    im_padded = pad_image((512, 512), im.shape, im)\n",
    "    im_zoomed = clipped_zoom(im_padded, 2)\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        os.remove(filename)\n",
    "    \n",
    "    new_image_path = f'{filename[:filename[1:].index(\".\")+1]}.png'\n",
    "    \n",
    "    plt.imsave(new_image_path, im_zoomed, cmap='gray')\n",
    "    print(f\"Saved new image to {new_image_path}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6f238897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(manual=False):\n",
    "    if not manual:\n",
    "        data = load_data('missing_chars.csv') # Import characters to scrape\n",
    "        print(data)\n",
    "        url_list = get_page_urls(scrape_url_base, data)\n",
    "    else:\n",
    "        url_list = manual_url\n",
    "        print(url_list)\n",
    "\n",
    "    for i in range(len(url_list)):\n",
    "        print(f'Searching for character {url_list[i][0]} index {url_list[i][2]}')\n",
    "        scrape_image(url_list[i][1], url_list[i][0], url_list[i][2])\n",
    "\n",
    "    \n",
    "main(manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3625dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR TESTING\n",
    "\n",
    "img = Image.open(os.path.join(data_dir, \"756\", \"756_1.gif\")).convert('L')\n",
    "    \n",
    "im = np.asarray(img)\n",
    "print(im.shape)\n",
    "print(im)\n",
    "\n",
    "# im is read-only\n",
    "im = im.copy()\n",
    "\n",
    "# remove watermark through thresholding\n",
    "thres = 140\n",
    "im[im > thres] = 255\n",
    "im[im <= thres] = 0\n",
    "\n",
    "im_padded = pad_image((512, 512), im.shape, im)\n",
    "\n",
    "# im_downsampled = np.zeros((im_padded.shape[0]//2, im_padded.shape[1]//2))\n",
    "\n",
    "# for i in range(im_padded.shape[0]):\n",
    "#     for j in range(im_padded.shape[1]):\n",
    "        \n",
    "#         im_downsampled[i//2, j//2] = im_padded[i, j]\n",
    "\n",
    "im_zoomed = clipped_zoom(im_padded, 2)\n",
    "\n",
    "plt.imshow(im_zoomed, cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
